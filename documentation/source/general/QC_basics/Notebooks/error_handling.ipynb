{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5966e0bd",
   "metadata": {},
   "source": [
    "# 404 Problem not found: Error Handling \n",
    "\n",
    "In previous chapters, we have mentioned that although some algorithms are theoretically developed, the implementation on real hardware poses enough problems for the algorithms to have no meaning (yet) in our everyday life. But why is that the case? And which solutions are being realized to better the reliance of quantum hardware? This chapter is dedicated to answering these questions. \n",
    "\n",
    "Building elaborate quantum computers comes with many challenges, which we will go through again. To hopefully achieve quantum advantage nevertheless, error handling techniques arised, so we can reduce these effects. In the third section, we will have a look at those, after going through the types of faults that can occur on quantum hardware. \n",
    "\n",
    "## Repetition: Challenges \n",
    "\n",
    "The error rate in current quantum computers is between 0.1% to 1%. This might not sound like a lot, but intricate circuits utilize hundreds to thousands of gates, where errors are almost guaranteed.   \n",
    "The reason we put so much effort into developing new algorithms, like these introduced in the previous chapters, is the promised quantum advantage, meaning that we can solve problems proven much faster on a quantum computer. \n",
    "However, there are still challenges we face before quantum advantage is reached, which we will explore now. \n",
    "\n",
    "**Connectivity** between qubits is essential. When two qubits need to be adressed together (e.g. with a CNOT gate), but are not connected to each other, the qubits need to be permuted via SWAP operations which costs valuable computation time and thus increases the probability of quantum noise effects (errors). However, a well-connected qubit system can lead to unwanted qubit interactions (cross-talk) which induce **decoherence**. The slightest change in temperature, humidity, electromagnetic influences (even in another room) or even the qubits themselves can lead to information loss. Of course, these challenges are kept in mind when designing the hardware, but it is still hard to completely isolate the qubits from the evironment, especially since we need to manipulate them via gate functions.   \n",
    "Similar, decoherence limits the **scalability**, as bigger quantum circuits offer more chances to induce noise. A higher qubit count however is needed to tackle the interesting problems.  \n",
    "\n",
    "These effects often prohibit as many qubit connections as mathematically or physically possible, or require additional qubits for error correction. \n",
    "This leads to the logical qubit to physical qubit trade-off.  You can therefore often only implement algorithms using fewer logical qubits than available physical qubits. This relationship is estimated to be 10:1 to 1.000:1, depending on the algorithm, the error correction method and the fault-tolerance of the computation. With Steane-Code for example, you can implement one logical qubit with seven physical ones, by using Hamming codes to correct qubit flip errors and phase flip errors. \n",
    "\n",
    "In classical computing, an easy way to avoid errors is to send the solution multiple times and go by a vote of majority if there are inconsistencies. Unfortunately, because of the no-cloning theorem and the nature of quantum mechanical particles, repeating experiments is not that easy, since you can't simply copy the qubit. You need to gather a new piece, prepare it and go through the entire algorithm to receive one result. This is repeated many times and a statistical mean is calculated, which is now used as your answer, to even out wrong result because of errors. This is, of course, computationally expensive. With improved error correction methods, you could dial down the number of experiments needed until you can be confident in your mean. Therefore, error correction methods that use clever tricks instead are detrimental in quantum computing. \n",
    "\n",
    "\n",
    "## Types of Errors \n",
    "\n",
    "There are different types of errors that can occur in a quantum computer, and the type determines if and how we can correct the error. \n",
    "\n",
    "**Depolarization errors** is the fancy way of saying \"we don't know what went wrong here\". It is simulated by a probability $p$ that the qubit faces a random Pauli error due to noise. To represent them, special matrices are used, so-called Kraus operators. They model the operation (also called quantum channels) between two quantum states. \n",
    "\n",
    "**Leakage**: quantum mechanical particles, such as an electron, can be in more states than our computational basis. Is an electron in state $\\ket 1$ randomly excited, it can leak outside our basis in the state $\\ket 2$ and can't be used for the calculation anymore. This case is special since it's hard to account for in error correction codes and may also corrupt neighboring qubits. \n",
    "\n",
    "Similar to classical computing, a qubit's whole value can flip, from 0 to 1 or the other way around. If you remember the physical implementation of qubits, this can happen when a particle  loses energy ( $\\ket 1 \\rightarrow \\ket 0$) or gets unintentionally excited ($\\ket 0 \\rightarrow \\ket 1$). Because this error mirrors the effect of a Pauli X gate, they are called **X errors**. As you can imagine, this kind of error is fatal to the calculation and has to be corrected in any case. In classical computing, these errors can be caught with e.g. Hamming codes (TODO: explain hamming codes?).  \n",
    "But since qubits carry more information that a classical bit, they are also prone to more errors. Additional to the value flip, the phase of the qubit can also flip. This could change the sign in a superposition from $\\frac{1}{\\sqrt 2} (\\ket 0 + \\ket 1)$ to $\\frac{1}{\\sqrt 2} (\\ket 0 - \\ket 1)$. As this error mimicks a Pauli Z gate, **Z errors** may not be important to the result, since it keeps the state $\\ket 0$ the same.   \n",
    "The last in the pack, the Pauli Y error, also mimicks its Pauli gate and changes $\\frac{1}{\\sqrt 2} (\\ket 0 + \\ket 1)$ to $\\frac{1}{\\sqrt 2} (-i\\ket 0 + i\\ket 1)$. A Pauli Y error can be seen as a mixture of Pauli X und Z, because it switches the values of $\\ket 0$ and $\\ket 1$ and additionally affects the phase.  \n",
    "The errors X, Y and Z form a basis, since the corresponding Pauli matrices form a mathematical basis that lets you express any other 2x2 matrix as a linear combination of them (similar to the Pauli gates). Therefore, any error on a qubit can be expressed as a combination for X, Y, and Z errors. This knowlegde is essential to building error correction codes. To deal with the errors occuring on actual hardware (which might be a mixture of the mentioned ones), we discretize errors like decoherence into small, singular Pauli errors, which are easier to account for mathematically. \n",
    "\n",
    "Do you have Wanderlust? So do atoms sometimes, but in stark contrast, atoms (and therefore qubits) can just dissapear. As you can imagine, losing a qubit messes up the whole calculation and renders the result unusable. Unfortunately, it was difficult to detect these errors without measuring and therefore collapsing the quantum system. But fear not, a new technique was recently discovered by the Sandia National Laboratories and the University of New Mexico that sidesteps this issue with the help of a Leakage-Detection Unit (LDU) with 93.4% accuracy. It detects atom loss by mapping the information onto a separate ancilla qubit, which acts like an ‘observer’ without disturbing the qubit. \n",
    "\n",
    "Another way to differentiate errors is by coherence. Interactions between qubits and their environment result in incoherent errors, which lower the purity and lead to decoherence. \n",
    "Contrarily, Coherent errors occur due to a used gate (systematic imperfections in qubit control, like detuning and calibration errors) and crosstalk on multi-qubit processors. These errors are purity-preserving and therefore do not result in decoherence, so they don't lose quantum information. Coherent errors add up over time, while incoherent one are unpredictable in a one-and-done fashion.\n",
    "Incoherent errors add up linearly, while coherent errors can accumulate quadratically. \n",
    "\n",
    "As we have seen, quantum computers are very prone to errors of all kinds. But luckily, there are different techniques to deal with them.\n",
    "\n",
    "## Types of Error correction \n",
    "\n",
    "Depending on when the error correction is applied, we differentiate different kinds: \n",
    "\n",
    "**Error suppression** deals with hardware-near errors, and is not limited to quantum computing, but also used by other quantum technologies. MRIs already use techniques for error suppression like refocusing of spin magnetisation by a pulse of resonant electromagnetic radiation (spin echoes) to lengthen coherence time. \n",
    "\n",
    "**Error mitigation** is applied at the compiling process to work on the software. It is important to note that the errors reduced by mitigation stem from the hardware, not the algorithm itself, but are treated on a software level. Error mitigation has the potential to approximate noise-free expectation values. \n",
    "Currenty, error mitigation is often prefered to error correction codes, since they generally have less overhead and are therefore easier to implement in short-depth circuits involving less qubits, suitable for the hardware of today.  This makes mitigation attractive for the current NISQ (noisy intermediate-scaling quantum) era, whose devices are not fully fault-tolerant (fault-tolerant quantum computers can safely carry out calculations even when errors are present). The specific size of overhead and accuracy depends on the chosen method.   \n",
    "Error mitigation reduces errors, but doesn't completely eradicate them. There are different methods to reduce errors, from which we will now introduce two popular ones. \n",
    "**Randomized Compiling** introduces random single gates into our algorithm by embedding a gate in the random gates from a so-called twirling set. The twirling sets are chosen in a way that doesn't influence the algorithm globally. This is repeated in different rounds for certain gates. Through this process, universal errors are fitted into stochastic Pauli errors. In doing so, potentially aggregated (quadratic) coherent errors are broken up and switched to stochastic Pauli errors. This method convinces with the advantage of little additional overhead.   \n",
    "With **Zero Noise Extrapolation**, we are essentially \"guessing\" what the circuit would produce without noise. For that, we first execute our algorithm at different noise levels. The second part is the actual extrapolation where we track based on our previous results, what we would get if we had close to zero noise. \n",
    "\n",
    "We can further divide errors into two groups: gate errors, that occur when manipulating the qubits, and readout errors, that relate to measuring the states. One technique in readout error mitigation (REM) is to apply inverted transition or confusion matrices to the noisy measurement results. These state preparation and measurement errors (SPAM) are statistically captured and a mitigation matrix is built based on these information with the goal to transform the measured, faulty data into the correct states based on probably errors. That way, you can detect faulty data from noise-free shots.   \n",
    "\n",
    "While error mitigation profits from relatively easy-to-implement techniques, it is used to reduce errors, not fully remove them. To potentially completely eliminate errors, **error correction** codes are used. In contrast to mitigation, error correction takes place during computation.  Because it is quite cost-intensive, error correction is not always used; especially for short algorithms on today's limited hardware it is not worth the effort.  \n",
    "\n",
    "The threshold theorem claims that as long as your physical errors for one gate application stay under a certain threshold, they can be compensated using logical error correction codes. This might not seem big, but the theorem states that even after an arbitrary length, the calculation doesn't get destroyed by accumulated noise. \n",
    "To get the threshold for your specific code, you need to calculate the probabily of an error occuring with and without error correction. You have reached the threshold when $p_{no error correction} > p_{with error correction}$. The surface code for error correction for example has a threshold of about 1%. A smaller threshold would be more problematic, since this requires a higher accuracy per gate and therefore more physical qubits to implement one logical one.   \n",
    "Unfortunately, error correction only makes sense to use below this threshold, which isn't reached by most machines today, which is why error correction isn't widely employed yet and we opt for error mitigation instead. \n",
    "\n",
    "Analogous to classical computing, error correction codes (QECC) were proposed to counteract gate errors. QECCs use redundancy to detect and correct errors by distributing information across multiple physical qubits. One QECC is the syndrome extraction. We start by measuring the so-called stabilizers, these can be the Pauli operators for example. \n",
    "As already established, cloning qubits is not possible. However, with a two-qubit encoder, we can distribute the information onto a two qubits state. Insead of $\\alpha \\ket{0} + \\beta \\ket{1}$, we encode $\\alpha \\ket{00} + \\beta \\ket{11}$. These entangled entities are put through the whole computation. Measuring the state $\\ket{01}$ or $\\ket{10}$ would hint a bit flip error $X_1$ (this method can therefore be used for error detection, but not error correction. We could encode $\\ket{0} \\rightarrow \\ket{000}$ and correct to the most likely initial state). The outcome constitutes the stabilizer measurement. Since this construction can detect bit flip errors, its stabilizer is the bit flip gate Pauli-X.  Choosing the Pauli-X operator as a stabilizer is quite effective, since it commutes with the rest of the ciruit and errors in it are easily detectable.\n",
    "To measure the stabilizer, a ancilla qubit is introduced. Is the ancilla bit measured to be $\\ket{0}$, no error is detected. The outcome of the ancilla qubit is considered the sydrome. Again, Pauli-X or -Z gates can be used to retrieve the sought-out state and correct the error. By measuring syndromes related to stabilizer operators, it allows quantum computers to identify and correct errors <sup>[1]</sup>.  \n",
    "\n",
    "At every step of the way there's the chance to implement QEC, like when encoding one logical qubit as multiple phsical qubits. Depending on how and how many qubits you connect, you can detect and correct different errors. These as the most popular methods: \n",
    "\n",
    "+ A simple repetition code is easy to implement and doesn't have to cause as much overhead as other methods. However, you can only detect bit flips, not phase errors. Can't always correct with certainty. \n",
    "\n",
    "+ Shor: The no-cloning theorem forbids blindly copying a qubit to others, but can be bypassed by entangling them and therefore making their value depend on each other. This is where a protocol by Peter Shor (yes, that Shor again) comes into play: He proposed that entangling 9 physical qubits could protect the information of one logical qubit without determining the actual state of it (and therefore collapsing the qubit). This code can correct a bit flip OR a phase flip by implementing 2 extra repetition qubits for correcting a bit flip via majority vote (which technically means hat if 2 qubits are compromised in the same fashion, we wouldn't notice and go for the error instead) and tripling each qubit again to correct phase flips. The second step has to be implemented via Hadamard rotations, since we can't simply copy qubits. With 9 qubits used, we have quite an overhead and wouldn't chose this method today, but historically it was the first QECC and is therefore worth mentioning. \n",
    "\n",
    "+ Steane Code can correct bit flips and phase flips. One logical qubit gets encoded into 7 physical ones. It's inspired by classical Hamming codes. The Steane code is a distance 3 code, meaning that it detects errors on 2 qubits and corrects errors on 1 qubit. TODO\n",
    "\n",
    "+ For the Surface Code, qubits are arranged in a grid where the outer boundaries connect, so it forms a torus(ring) in three dimensional space. There are different flavours of Surface Codes, depending on the stabilizers you use. Surface Codes are the newest development and the hottest choice, also because of its high threshold of 1%. \n",
    " \n",
    "Error correction has a lot of potential, as the threshold theorem explains: Under certain assumptions of quantum noise, a quantum computer is able to perform an arbitrary quantum computation provided the error rate per physical gate or time step is below some constant threshold value. The exact threshold value for a QEC code depends strongly on the assumptions made on the quantum noise and is not fixed. While it might sounds disencouraging that we need to get under a certain threshold first, it also shows that fault-tolerance is achievable. Notice how this theorem only holds up for error correction, not mitigation; which is one of the reasons why researchers believe that correction will be more relevant than mitigation as we dive deeper into noise tolerant devices, while mitigation stays relevant for today's NISQ era. Moving into the future, error correction will have a more important role, since it's often seen as the key to achieving fault tolerant computing: meaning a quantum computer can continue to operate correctly even when individual operations or qubits error out. \n",
    "\n",
    "## Summary \n",
    "\n",
    "+ The error rate in current quantum computers is between 0.1% to 1%. \n",
    "+ Pauli errors (bit flip/phase flip), depolarization errors with probability $p$ and leakage (losing electrons) are main error categories \n",
    "+ incoherent (interaction with environment, losing purity) vs coherent (preparation/execution of gates, quadratic growth, keeping purity) errors \n",
    "+ Error surpression (hardware level), error correction (during computations) and error mitigation (reduce errors in results) are all ongoing research fields to better the accuracy of quantum computers.\n",
    "+ two popular ways to implement error mitigation are zero-noise extrapolation and random compiling \n",
    "+ threshold theorem: Under certain assumptions of quantum noise, a quantum computer is able to perform an arbitrary quantum computation provided the error rate per physical gate or time step is below some constant threshold value.\n",
    "\n",
    "\n",
    "<sup>[1]</sup>This is a very brief explanation. For more details, we recommend this paper: Roffe, Joschka. \"Quantum error correction: an introductory guide.\" Contemporary Physics 60.3 (2019): 226-245."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
